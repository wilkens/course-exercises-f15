{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP and entity extraction\n",
    "## Week 10\n",
    "### Summary\n",
    "\n",
    "Use the Stanford NLP tools to identify named entities in a short text, then write a Python program to parse the NLP output.\n",
    "\n",
    "### Details\n",
    "\n",
    "The text in question is one we've already seen (in the sutructured data exercise). To review: it's a brief pamphlet, *Growler's Income Tax* (1864), by the prolific mid-nineteenth-century writer T.S. Arthur. It's a defense of the then-new income tax, instituted in 1861 to fund the Union's war effort. As you'll see, the text is pretty straightforward, but it's kind of nifty (or infuriating, I guess) how similar are the arguments it presents concerning taxation to those you might hear today. Go ahead, download the [plain-text copy](https://github.com/wilkens/course-exercises-f15/blob/master/arthur.txt) and read it now, if you haven't already. It's short. (Note that you could also work with the [XML version](https://github.com/wilkens/course-exercises-f15/blob/master/growler.xml) from the previous exercise and convert that to plain text as shown in the [solution to that exercise](https://github.com/wilkens/course-exercises-f15/blob/master/Parsing%20structured%20data%20solution.ipynb).)\n",
    "\n",
    "Anyway, tax policy isn't really the point. Your task is to identify algorithmically the named entities in the text and to extract them for further processing. To do this, you'll use the Stanford NLP toolset. One possibility would be to use the full CoreNLP package, with which you can do much more than identify named entities (for example, you can do part of speech tagging, co-reference resolution, and sentiment analysis) in a single pass over a given text. But the simplest approach is to use just the [NER tool](http://nlp.stanford.edu/software/CRF-NER.shtml).\n",
    "\n",
    "The NER tool, like the full CoreNLP package, is a Java program. That means you’ll need to have Java on your computer. And not just the Java runtime for your browser; you need the full [Java Development Kit](http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html). Make sure you get the proper version for your system (OS and 32/64-bit; you do not need the \"Demos and Samples Downloads\"). You can check to see if you have Java installed by opening a terminal and typing `java -version`. If you get something like these lines (the Stanford tools require version 1.8 or higher), you’re good:\n",
    "\n",
    "```\n",
    "java version \"1.8.0_60\"\n",
    "Java(TM) SE Runtime Environment (build 1.8.0_60-b27)\n",
    "Java HotSpot(TM) 64-Bit Server VM (build 25.60-b23, mixed mode)\n",
    "```\n",
    "\n",
    "Follow the instructions included with the NER download ( see `README.txt`) to process the input text. Note that there are several output formats supported. I'd suggest using `-outputFormat inlineXML`, which I believe is the default. Save your output to an appropriately named file.\n",
    "\n",
    "Now write some Python code that reads the NER output file and builds a list of unique entities in the output, each entity's type, and a count of how many times each entity occurs.\n",
    "\n",
    "Your program should print a summary of this data to standard output and write a CSV file named `entities.csv` that contains the same information. Your terminal output (with made-up data) should look roughly like this:\n",
    "\n",
    "```\n",
    "Entity\t\tType\t\tCount\n",
    "------\t\t----\t\t-----\n",
    "Boston\t\tLocation\t2\n",
    "John Smith\tPerson\t\t1\n",
    "```\n",
    "\n",
    "Your CSV file should have the same structure, but no fake underlining and no tabs, with entities separated by commas. For an optional challenge, make sure your output can accommodate entity strings that include commas (probably by making sure all entity strings are enclosed by quotation marks or other markers that indicate the beginning and end of strings).\n",
    "\n",
    "### Alternative processing\n",
    "If you're up for a modest challenge, you might try to invoke the the NER tool from within Python using the NLTK module (see the [`nltk.tag.stanford`](http://www.nltk.org/api/nltk.tag.html#module-nltk.tag.stanford) docs) rather than from the command line. Or, if you really want, you could skip the Stanford tools entirely and use NLTK's (slower, lower-performing, but pure Python) named entity chunker. But both ideas are strictly optional.\n",
    "\n",
    "### Submit\n",
    "\n",
    "Submit three files via Sakai: your Python code as an iPython notebook, your NER output (in whatever format, probably ill-formed XML), and your CSV entity output.\n",
    "\n",
    "### Consider\n",
    "\n",
    "A few things to think about before class:\n",
    "\n",
    "* How well or poorly do the entities extracted from the text square with your sense of what the text is about, whom it involves, and where it occurs (or with what areas it’s concerned)?\n",
    "* How accurate is the NER process?\n",
    "* How might you try to improve NER accuracy?\n",
    "\n",
    "## Starter code\n",
    "\n",
    "Here's a bit of code to get you going ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "input_name =  \"arthur-ner.xml\"      # File names for input and output\n",
    "output_name = \"entities.csv\"\n",
    "\n",
    "with open(input_name, 'r') as f:\n",
    "    soup = BeautifulSoup(f, 'lxml')\n",
    "    #print(soup.prettify()) # Can use this to examine the parsed file\n",
    "    \n",
    "    locations = {}  # Dictionaries to hold found entities and their counts\n",
    "    people    = {}  # Could also use a dict of dicts.\n",
    "    orgs      = {}\n",
    "    \n",
    "    for i in soup.p.find_all(): # NB. finding subtags of 'p', inserted by BeautifulSoup\n",
    "        entity_name = i.get_text()\n",
    "        entity_type = i.name\n",
    "        \n",
    "        ## Your code here ##\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity\t\tType\tCount\n",
      "------\t\t----\t-----\n",
      "Pennsylvania \t location \t 2\n",
      "Gettysburg \t location \t 1\n",
      "Philadelphia \t location \t 1\n",
      "Rappahannock \t location \t 1\n",
      "Delaware \t location \t 1\n",
      "France \t location \t 1\n",
      "Florida \t location \t 1\n",
      "Carlisle \t location \t 1\n",
      "Alabama \t location \t 1\n",
      "District \t location \t 1\n",
      "England \t location \t 1\n",
      "Lee \t person \t 2\n",
      "JOHN M. RILEY \t person \t 2\n",
      "ARTHUR \t person \t 1\n",
      "Growler \t person \t 1\n",
      "Riley \t person \t 1\n"
     ]
    }
   ],
   "source": [
    "# The remaining code ...\n",
    "def incrementEntity(entity_string, dictionary):\n",
    "    \"\"\" Examines dict 'dictionary' for key 'entity_string'.\n",
    "        If key exists, increments counter value, else creates key and\n",
    "        assigns value 1.\"\"\"\n",
    "    try:\n",
    "        dictionary[entity_string] += 1\n",
    "    except KeyError:\n",
    "        dictionary[entity_string] = 1\n",
    "\n",
    "def outputResults(dictionary, entity_type, f):\n",
    "    \"\"\" Takes input dict 'dictionary', str 'entity_type',\n",
    "        and csv.writer pointer 'f'.\n",
    "        Sorts dictionary keys by values highest to lowest, and writes\n",
    "        formatted output to CSV and to screen.\"\"\"\n",
    "    for i in sorted(dictionary, key=dictionary.get, reverse=True):\n",
    "        print(i, '\\t', entity_type, '\\t', dictionary[i])\n",
    "        f.writerow([i, entity_type, dictionary[i]])\n",
    "\n",
    "# Re-initialize variables\n",
    "locations = {}  # Dictionaries to hold found entities and their counts\n",
    "people    = {}  # Could also use a dict of dicts.\n",
    "orgs      = {}\n",
    "\n",
    "# Loop over all tags in input file. Assumes only NER tags are present.\n",
    "for i in soup.p.find_all():\n",
    "    entity_name = i.get_text()\n",
    "    entity_type = i.name\n",
    "    # Note code below assumes we know the full list of entity types\n",
    "    #  More versatile would be to build a dict of dicts, i.e.:\n",
    "    #   entities = {\"London\":{\"location\":250, \"person\":3}}\n",
    "    #  and then iterate over that. Meh, overkill here.\n",
    "    if entity_type == 'location':\n",
    "        incrementEntity(entity_name, locations)\n",
    "    elif entity_type == 'person':\n",
    "        incrementEntity(entity_name, people)\n",
    "    elif entity_type == 'organization':\n",
    "        incrementEntity(entity_name, orgs)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "with open(output_name, 'w') as output_file:\n",
    "    f = csv.writer(output_file)\n",
    "    print(\"Entity\\t\\tType\\tCount\")\n",
    "    print(\"------\\t\\t----\\t-----\")\n",
    "    f.writerow([\"Entity\", \"Type\", \"Count\"])\n",
    "\n",
    "    outputResults(locations, 'location', f)\n",
    "    outputResults(people, 'person', f)\n",
    "    outputResults(orgs, 'organization', f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The NLTK version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mwilkens/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/utils/fixes.py:64: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  if 'order' in inspect.getargspec(np.copy)[0]:\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.chunk import tree2conlltags, conlltags2tree\n",
    "from nltk import pos_tag, ne_chunk\n",
    "from collections import defaultdict\n",
    "\n",
    "counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "with open('arthur.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "    sentences = sent_tokenize(text)\n",
    "    for sent in sentences:\n",
    "        tokens = word_tokenize(sent)\n",
    "        pos = pos_tag(tokens)\n",
    "        ne = ne_chunk(pos)\n",
    "        ne_tags = tree2conlltags(ne)\n",
    "        for entity in ne_tags:\n",
    "            if entity[2] != 'O':\n",
    "                counts[entity[2]][entity[0]] += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " B-GPE\n",
      "   Pennsylvania 2\n",
      "   Southern 1\n",
      "   ARTHUR 1\n",
      "   France 1\n",
      "   Carlisle 1\n",
      "   GROWLER 1\n",
      "   District 1\n",
      "   American 1\n",
      "   Income 1\n",
      "   Gettysburg 1\n",
      "   South 1\n",
      "   Rappahannock 1\n",
      "   Collector 1\n",
      "   Florida 1\n",
      "   Stand 1\n",
      "   England 1\n",
      "\n",
      " B-GSP\n",
      "   Philadelphia 1\n",
      "\n",
      " B-ORGANIZATION\n",
      "   INCOME 1\n",
      "   Assessor 1\n",
      "   Delaware 1\n",
      "   RICHARD 1\n",
      "   Alabama 1\n",
      "   Union 1\n",
      "   Income 1\n",
      "\n",
      " B-PERSON\n",
      "   Growler 13\n",
      "   Lee 2\n",
      "   JOHN 2\n",
      "   Collector 2\n",
      "   Tax 1\n",
      "\n",
      " I-ORGANIZATION\n",
      "   Tax 1\n",
      "\n",
      " I-PERSON\n",
      "   M. 2\n",
      "   RILEY 2\n",
      "   Riley 1\n",
      "   Revenue 1\n",
      "   Internal 1\n"
     ]
    }
   ],
   "source": [
    "for i in sorted(counts.keys()):\n",
    "    print('\\n',i)\n",
    "    for j in sorted(counts[i], key=counts[i].get, reverse=True):\n",
    "        print('  ', j, counts[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
